{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.4 합성곱/풀링 계층 구현하기\n",
    "## 7.4.1 4차원 배열\n",
    ": CNN 계층 사이를 흐르는 데이터<br/>\n",
    ": (10, 1, 28, 28) - 데이터, 채널, 높이, 너비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 28, 28)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.rand(10, 1, 28, 28)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.86018757e-01, 1.42138738e-01, 3.32036445e-01, 2.85604740e-01,\n",
       "        2.57397724e-01, 3.36001433e-02, 9.87256824e-01, 7.42138639e-01,\n",
       "        7.29441008e-01, 1.85726016e-01, 2.20151476e-01, 4.15639416e-01,\n",
       "        4.68282968e-01, 3.76265338e-01, 5.17840724e-01, 3.87685467e-02,\n",
       "        9.20892037e-01, 9.67828269e-01, 9.28074092e-01, 1.83845433e-01,\n",
       "        9.69215244e-01, 6.89370746e-01, 8.04499676e-01, 2.71983591e-01,\n",
       "        3.64934225e-01, 6.82068216e-01, 8.92972974e-01, 2.02072531e-03],\n",
       "       [3.14793180e-01, 3.32309933e-01, 1.68068502e-02, 7.88918637e-02,\n",
       "        6.23639350e-01, 3.99630516e-01, 8.76649412e-01, 3.14618576e-02,\n",
       "        7.42605171e-01, 7.15416588e-01, 4.51483556e-01, 4.88443436e-01,\n",
       "        7.72872851e-01, 4.65353770e-02, 4.90596022e-01, 4.18615941e-01,\n",
       "        2.95825447e-01, 7.11577559e-01, 7.08767520e-01, 2.19670946e-02,\n",
       "        2.81752662e-01, 2.71207208e-01, 5.63484390e-01, 9.89304168e-01,\n",
       "        7.79310461e-01, 8.60788485e-01, 4.28998612e-01, 2.34949521e-01],\n",
       "       [6.79052765e-01, 9.39927478e-01, 9.69162824e-01, 1.42438010e-01,\n",
       "        5.99233015e-01, 8.35974995e-01, 2.63670992e-01, 2.43155959e-01,\n",
       "        3.41343993e-01, 9.49276335e-01, 7.32563009e-01, 1.46424482e-01,\n",
       "        5.62462542e-01, 8.96335501e-02, 1.14445303e-01, 9.18491516e-01,\n",
       "        3.69656709e-01, 5.55492651e-01, 7.51537960e-01, 3.72626995e-01,\n",
       "        2.19246895e-01, 1.71914154e-01, 2.79507296e-01, 3.84589367e-01,\n",
       "        7.42220625e-01, 7.17504501e-01, 4.05604234e-01, 5.61518840e-01],\n",
       "       [1.50481884e-01, 6.03500815e-01, 9.97876077e-01, 2.29219975e-01,\n",
       "        2.85474238e-01, 3.58829914e-01, 4.17486941e-01, 7.03997688e-01,\n",
       "        8.78305118e-01, 6.79400897e-01, 4.19156466e-02, 1.05619673e-01,\n",
       "        1.42977513e-01, 8.20570042e-01, 8.44137180e-01, 4.08513627e-01,\n",
       "        5.52828430e-01, 2.26639132e-01, 2.28268319e-01, 2.06608137e-01,\n",
       "        7.19546701e-01, 6.84070206e-01, 8.08057086e-01, 9.24058362e-01,\n",
       "        7.55998907e-02, 5.96549691e-01, 7.45650466e-01, 1.19553537e-01],\n",
       "       [1.01722783e-01, 4.78975936e-01, 7.70928434e-02, 2.71746759e-01,\n",
       "        9.72000435e-01, 7.43247996e-01, 7.11412477e-01, 7.04789883e-02,\n",
       "        7.40075379e-01, 2.18872221e-01, 6.65385418e-02, 4.15252288e-02,\n",
       "        6.43903617e-01, 7.54233570e-01, 6.09784723e-01, 4.52183129e-01,\n",
       "        8.31053809e-01, 9.24148024e-01, 6.37597695e-01, 2.06872992e-01,\n",
       "        8.55282167e-01, 1.11790039e-03, 3.05236232e-02, 6.07894486e-01,\n",
       "        2.21831238e-01, 7.74400251e-01, 6.89823854e-01, 1.95139887e-01],\n",
       "       [3.86333042e-01, 9.49221344e-01, 5.99210926e-02, 9.55054823e-01,\n",
       "        7.63427864e-01, 7.34419345e-01, 2.37159441e-01, 6.06793407e-01,\n",
       "        5.58351730e-01, 9.60489474e-01, 3.99911248e-01, 9.83444361e-01,\n",
       "        1.75046317e-01, 7.26398340e-01, 1.30623467e-01, 5.86250792e-01,\n",
       "        7.17281082e-01, 9.07092200e-01, 7.37964184e-01, 4.22669279e-01,\n",
       "        3.09807606e-01, 2.31375932e-01, 2.63782957e-01, 9.25759616e-01,\n",
       "        3.97882710e-02, 3.59162655e-01, 4.38131640e-01, 3.62592357e-01],\n",
       "       [2.95149579e-03, 5.80245723e-01, 8.48572212e-01, 2.91028940e-01,\n",
       "        2.20019397e-01, 2.38329362e-01, 6.35718134e-01, 4.36963239e-02,\n",
       "        1.95674023e-02, 4.86261570e-01, 4.04457679e-01, 8.66613333e-01,\n",
       "        5.43454753e-01, 6.53609962e-01, 1.93588364e-01, 8.66648505e-01,\n",
       "        6.32919017e-01, 5.26477691e-01, 3.25320867e-01, 5.11731641e-01,\n",
       "        7.81569619e-01, 4.37153725e-01, 6.67373167e-01, 6.69736575e-01,\n",
       "        5.82353186e-01, 8.77038933e-01, 2.67947970e-01, 8.48928083e-01],\n",
       "       [2.97166624e-01, 9.26051021e-01, 1.67407968e-02, 8.75432900e-01,\n",
       "        2.57297766e-01, 7.04568558e-01, 3.15695056e-01, 3.87519538e-01,\n",
       "        8.85175924e-01, 5.41269071e-01, 7.76755923e-01, 5.44822018e-01,\n",
       "        8.35691338e-02, 3.11828916e-01, 1.21153430e-01, 8.84824082e-02,\n",
       "        1.43201144e-01, 9.70200271e-02, 7.80471290e-01, 4.98510919e-01,\n",
       "        1.43579706e-01, 6.81095573e-01, 1.60029513e-01, 7.80277495e-01,\n",
       "        3.66191048e-01, 5.45598078e-01, 7.44359308e-01, 5.07033467e-01],\n",
       "       [4.28536372e-01, 3.75917904e-01, 3.55272624e-01, 1.70784910e-01,\n",
       "        3.86438848e-01, 8.61762163e-01, 6.22280068e-01, 6.98350592e-02,\n",
       "        8.53663420e-02, 8.83441282e-01, 1.76090305e-01, 4.25151045e-01,\n",
       "        4.56741832e-01, 3.00717719e-01, 3.48882439e-01, 9.72163313e-01,\n",
       "        7.41107403e-01, 8.78305668e-01, 5.55503167e-02, 6.74298788e-01,\n",
       "        7.58038378e-01, 9.71812406e-01, 5.66294899e-01, 4.85939308e-01,\n",
       "        6.62010723e-01, 5.83820962e-01, 2.19928684e-01, 8.56924050e-01],\n",
       "       [4.21889247e-01, 1.40682652e-01, 2.14216872e-01, 1.41263550e-02,\n",
       "        7.36854942e-01, 1.79834288e-02, 7.26604563e-01, 3.23588180e-01,\n",
       "        8.35676594e-01, 4.68492574e-01, 5.58548819e-01, 7.74255510e-01,\n",
       "        4.92060391e-01, 5.17531700e-01, 4.47395353e-02, 9.32401288e-01,\n",
       "        4.32280298e-01, 8.27295644e-01, 6.13238261e-02, 5.38582099e-01,\n",
       "        1.01206264e-01, 4.04519026e-01, 2.23647306e-01, 4.65471798e-01,\n",
       "        7.48616718e-01, 4.76617318e-01, 3.44931373e-01, 8.96834901e-01],\n",
       "       [6.95091504e-02, 9.08391563e-01, 2.66142131e-01, 1.04069282e-02,\n",
       "        8.19277949e-01, 4.71205550e-01, 6.74788828e-01, 3.00188970e-01,\n",
       "        1.24241281e-02, 8.68799318e-02, 7.44489387e-01, 3.59059508e-01,\n",
       "        8.00897672e-01, 3.61310752e-01, 6.21834442e-01, 6.70470024e-01,\n",
       "        9.98949680e-02, 9.88733839e-01, 3.07086410e-01, 6.88224657e-02,\n",
       "        3.47042818e-01, 7.65825796e-01, 7.35440813e-01, 4.76665250e-01,\n",
       "        3.81715108e-01, 3.27083309e-01, 2.48551785e-01, 1.98344476e-01],\n",
       "       [1.34497094e-01, 8.07156843e-01, 3.71067668e-01, 2.90773970e-01,\n",
       "        6.62111631e-01, 3.88161389e-01, 4.79688882e-01, 3.09048815e-01,\n",
       "        8.73511758e-01, 4.19106584e-01, 5.34631031e-01, 9.68095188e-01,\n",
       "        9.66215581e-01, 2.95463818e-01, 6.41492444e-01, 1.09837783e-01,\n",
       "        3.76142457e-01, 7.73744149e-01, 4.49920511e-01, 1.73391488e-01,\n",
       "        1.43683926e-01, 7.73628472e-01, 7.02913961e-01, 2.35042874e-01,\n",
       "        1.95826480e-01, 3.91820739e-01, 5.68546857e-01, 5.39972534e-01],\n",
       "       [8.65078290e-01, 6.10095096e-01, 9.57654208e-03, 4.85008511e-01,\n",
       "        7.09918324e-01, 9.37142373e-01, 2.93513314e-03, 9.86823418e-02,\n",
       "        5.57703883e-01, 5.01259475e-01, 6.67120820e-01, 4.81980961e-01,\n",
       "        4.62868145e-01, 2.34434813e-01, 6.54087808e-01, 6.56256862e-01,\n",
       "        6.99600279e-01, 1.89473761e-01, 6.50542459e-01, 6.76600144e-01,\n",
       "        8.30262669e-01, 4.55748891e-01, 6.37210695e-01, 8.56329659e-01,\n",
       "        5.49328029e-01, 1.05753546e-01, 7.13310506e-01, 6.09839140e-01],\n",
       "       [5.29802516e-01, 6.05743917e-01, 3.29788328e-01, 6.59666159e-01,\n",
       "        3.02009663e-01, 4.82001267e-01, 1.26629337e-01, 6.50123652e-01,\n",
       "        7.45683959e-01, 7.84864277e-01, 2.54667638e-01, 2.60629489e-01,\n",
       "        9.58690490e-01, 8.85481527e-01, 2.20969852e-01, 7.68595860e-01,\n",
       "        6.07004303e-01, 9.89845501e-01, 8.83837776e-01, 8.80424722e-01,\n",
       "        2.26939685e-01, 2.41047875e-01, 7.42988595e-01, 9.91249906e-01,\n",
       "        6.32684945e-03, 5.93927300e-01, 6.74833584e-01, 1.08897073e-01],\n",
       "       [3.47681952e-01, 6.37665658e-01, 1.90854876e-01, 5.53775626e-01,\n",
       "        1.69537431e-01, 8.85184749e-01, 5.47741865e-01, 4.28484043e-01,\n",
       "        9.25954637e-01, 7.58624921e-01, 5.02296148e-01, 3.29616982e-02,\n",
       "        5.42033955e-01, 4.70130931e-01, 6.45091978e-01, 3.72974833e-02,\n",
       "        8.32495698e-01, 2.96455095e-01, 3.59146596e-01, 2.85597791e-01,\n",
       "        1.13637744e-01, 3.27767004e-01, 2.80433677e-01, 4.79421698e-01,\n",
       "        2.60253134e-01, 8.03791776e-01, 9.29944766e-01, 2.27849797e-01],\n",
       "       [8.14704415e-01, 6.61566059e-01, 5.35881456e-01, 2.04392995e-01,\n",
       "        5.79777081e-01, 7.54201092e-01, 2.29547213e-01, 2.87508378e-01,\n",
       "        4.49126015e-01, 8.45572117e-01, 3.57770571e-01, 3.88064447e-01,\n",
       "        7.65996984e-02, 2.27226584e-01, 8.41800671e-01, 4.25817847e-01,\n",
       "        2.35762258e-01, 3.91796346e-01, 7.36098085e-01, 2.88400725e-02,\n",
       "        1.15070896e-01, 8.45008976e-04, 5.91115121e-01, 8.93191281e-02,\n",
       "        3.88274730e-01, 3.16774180e-02, 3.50676887e-01, 6.24295167e-01],\n",
       "       [2.59080160e-01, 9.83969520e-01, 1.45210259e-01, 4.73883822e-01,\n",
       "        2.39473367e-01, 7.66376867e-01, 9.48566309e-01, 7.29414844e-01,\n",
       "        2.27018537e-01, 1.70846902e-01, 5.70171942e-01, 5.41999666e-01,\n",
       "        4.15528954e-01, 5.88721689e-01, 7.74427395e-01, 8.08310267e-01,\n",
       "        8.39141508e-02, 5.03277944e-01, 5.57225240e-01, 1.31389214e-01,\n",
       "        1.57936207e-01, 1.94219603e-01, 9.12342009e-01, 2.17608180e-01,\n",
       "        7.09061923e-01, 4.84389550e-01, 6.77197531e-01, 5.95479586e-01],\n",
       "       [3.71580239e-01, 3.51939004e-01, 4.45853476e-01, 5.02553579e-01,\n",
       "        4.88576141e-01, 9.06590685e-02, 9.60510809e-01, 3.29216884e-01,\n",
       "        6.32094406e-01, 3.62665865e-01, 7.24501340e-01, 3.95028257e-01,\n",
       "        9.02591161e-01, 4.46950682e-01, 3.44893127e-01, 4.78179716e-01,\n",
       "        7.01508260e-01, 5.25114858e-01, 4.05712954e-01, 1.68115289e-01,\n",
       "        6.32217871e-01, 7.32912771e-01, 6.00226842e-01, 4.77197472e-01,\n",
       "        2.11999288e-01, 6.71794070e-01, 2.53722422e-01, 2.85285800e-01],\n",
       "       [5.90168312e-01, 6.10313952e-01, 6.92121422e-01, 5.90338574e-01,\n",
       "        8.91851148e-01, 7.51025767e-01, 5.99687249e-01, 4.19896980e-02,\n",
       "        1.48326722e-01, 9.52339517e-01, 8.84418807e-01, 7.24266849e-01,\n",
       "        3.90077569e-01, 8.69013784e-01, 9.69826528e-01, 7.43425595e-01,\n",
       "        2.41050304e-01, 1.40703027e-02, 1.37988198e-01, 7.36025745e-02,\n",
       "        7.55545815e-02, 8.48862271e-01, 3.00988092e-01, 5.94404542e-01,\n",
       "        9.00523270e-01, 9.43266188e-01, 6.67784848e-01, 4.67009780e-01],\n",
       "       [5.50088186e-02, 9.78319358e-01, 5.81125591e-02, 8.29309635e-01,\n",
       "        6.47461759e-01, 5.77877181e-01, 2.06905068e-01, 1.60629360e-01,\n",
       "        7.77306285e-01, 5.08725231e-01, 8.69588787e-01, 6.90614236e-01,\n",
       "        3.13442602e-02, 9.96939951e-01, 5.07599327e-01, 2.36499612e-01,\n",
       "        4.12112661e-01, 2.94093701e-01, 8.27914850e-01, 9.01462400e-01,\n",
       "        8.77960096e-01, 5.05606436e-01, 4.80830588e-01, 9.58750703e-01,\n",
       "        1.14482153e-01, 4.05696841e-01, 8.20759845e-01, 1.68146696e-02],\n",
       "       [2.10003383e-01, 5.40283336e-01, 3.63849623e-01, 9.40116093e-01,\n",
       "        9.55817647e-01, 9.10993831e-01, 8.05591521e-01, 1.77258339e-01,\n",
       "        1.56772717e-01, 9.97059788e-01, 8.72283201e-01, 1.17500385e-01,\n",
       "        3.57410875e-01, 7.75625972e-01, 8.25420955e-01, 1.27336233e-01,\n",
       "        6.00053277e-01, 3.12054290e-01, 9.63855945e-01, 3.82805515e-01,\n",
       "        5.53363612e-01, 1.54085682e-01, 8.62179592e-01, 8.94408767e-01,\n",
       "        2.68422674e-01, 6.03263650e-01, 7.91734313e-01, 7.70670161e-01],\n",
       "       [2.50031669e-01, 8.33797326e-01, 4.40289257e-01, 5.53703945e-01,\n",
       "        5.71336373e-02, 3.69562033e-01, 8.67360707e-01, 9.76258444e-01,\n",
       "        5.89028458e-01, 2.00666347e-01, 1.33463015e-01, 4.38441983e-01,\n",
       "        8.30895451e-01, 1.44445015e-01, 1.43188565e-01, 1.46463145e-01,\n",
       "        7.88027845e-01, 7.03474218e-01, 6.54507613e-01, 6.52593994e-01,\n",
       "        1.23025934e-01, 9.06730141e-01, 9.53007458e-01, 3.69314609e-01,\n",
       "        7.69451878e-01, 9.40667720e-01, 7.83247360e-01, 9.95260297e-01],\n",
       "       [5.06245710e-02, 4.92769407e-01, 2.48278441e-01, 9.09548486e-01,\n",
       "        4.51323655e-01, 4.86721709e-02, 6.76479821e-01, 6.68099573e-01,\n",
       "        8.27833976e-01, 7.35173285e-01, 8.98976366e-01, 2.42405776e-01,\n",
       "        9.93376776e-01, 2.29334068e-01, 4.71875405e-01, 2.27331869e-01,\n",
       "        1.18304683e-01, 2.41412042e-01, 2.84551475e-01, 9.97538235e-01,\n",
       "        2.71042150e-01, 2.49870439e-01, 7.80817206e-01, 6.32740121e-01,\n",
       "        9.89957735e-01, 9.00256581e-01, 7.25808719e-01, 1.51556196e-01],\n",
       "       [8.60636452e-01, 4.33071507e-01, 5.82562169e-01, 9.93777260e-01,\n",
       "        4.15276726e-01, 2.83823660e-02, 3.70613885e-01, 2.60005911e-01,\n",
       "        8.56198933e-01, 4.02604113e-01, 7.38220690e-01, 2.98494182e-01,\n",
       "        5.01883664e-01, 2.61094473e-01, 4.21885189e-02, 7.12417411e-01,\n",
       "        7.88063289e-01, 2.72797017e-01, 5.23493798e-01, 9.93609134e-01,\n",
       "        1.52477218e-01, 1.08514955e-01, 1.87138695e-01, 6.27096979e-01,\n",
       "        4.95855266e-01, 5.38340948e-01, 5.37823755e-01, 4.14452901e-01],\n",
       "       [5.38391792e-01, 4.35605769e-01, 6.21770150e-01, 8.24955108e-01,\n",
       "        4.69347246e-01, 2.51255931e-01, 2.51262862e-01, 4.48332559e-02,\n",
       "        2.08152730e-01, 8.15608311e-01, 2.84765660e-01, 4.08953877e-01,\n",
       "        1.74183016e-01, 4.69468923e-01, 4.82801314e-01, 6.95635257e-01,\n",
       "        9.09465945e-01, 7.81854236e-01, 3.42435805e-01, 2.41357695e-01,\n",
       "        9.06823256e-01, 7.15198596e-01, 9.78085752e-01, 5.95745741e-01,\n",
       "        2.64020098e-01, 9.61834493e-01, 2.12276312e-01, 6.49745303e-01],\n",
       "       [7.66314187e-01, 4.95431827e-01, 3.91654610e-02, 9.34762699e-01,\n",
       "        8.96474635e-01, 8.55552668e-01, 4.14372523e-01, 9.89403543e-01,\n",
       "        5.54887427e-01, 7.16082801e-01, 9.12825102e-01, 7.14714708e-01,\n",
       "        2.09619937e-01, 7.45650112e-01, 5.79092298e-01, 7.80290296e-01,\n",
       "        8.37663912e-01, 1.61354738e-01, 1.44787803e-01, 9.41609466e-01,\n",
       "        8.75157903e-01, 4.54380281e-01, 9.13359620e-01, 7.53224411e-01,\n",
       "        8.75189073e-01, 9.32961975e-01, 1.70879586e-01, 8.56188977e-01],\n",
       "       [2.65569117e-01, 2.46772866e-01, 1.03870047e-01, 8.66904641e-01,\n",
       "        7.79363523e-01, 1.10021104e-01, 8.98562158e-01, 6.84771693e-01,\n",
       "        4.09704130e-01, 6.60359686e-01, 2.64878807e-01, 4.15911607e-01,\n",
       "        3.49317514e-01, 2.40962136e-01, 1.93321998e-01, 1.47540244e-02,\n",
       "        6.56027191e-01, 7.78681189e-01, 3.61084195e-01, 1.82752244e-01,\n",
       "        4.49463203e-01, 1.50815432e-01, 3.91797598e-01, 1.25281712e-01,\n",
       "        3.94646543e-01, 8.91122799e-01, 5.30961104e-01, 1.87990928e-01],\n",
       "       [2.16691116e-01, 2.35192654e-01, 2.04269569e-01, 3.79809791e-01,\n",
       "        4.56227373e-01, 1.79537175e-01, 8.42629320e-01, 6.95196267e-01,\n",
       "        6.23237939e-01, 7.27255906e-01, 5.01803847e-01, 5.63740476e-01,\n",
       "        6.08985902e-02, 6.53679211e-01, 3.70064892e-01, 2.92483075e-01,\n",
       "        5.68654988e-01, 2.13533168e-01, 5.48684339e-01, 7.26474473e-01,\n",
       "        8.63679253e-01, 5.03548393e-01, 2.81656648e-01, 4.17478043e-02,\n",
       "        3.68218827e-01, 9.17154053e-01, 1.03409156e-01, 8.57954502e-01]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.2 im2col로 데이터 전개하기\n",
    ": 입력데이터를 필터링하기 좋게 전개하는 함수<br/>\n",
    ": Numpy에서는 for문을 통한 원소 접근은 바람직하지 않다.<br/>\n",
    ": 필터를 적용하는 영역을 2차원으로 전개<br/>\n",
    ": 영역이 겹치는 경우 전개했을 때, 원소 수가 증가해서 메모리를 많이 소비하는 단점이 있다.<br/>\n",
    ": 하지만 행렬 계산 라이브러리 때문에 연산을 빨리할 수 있고, 효율을 높일 수 있다.<br/>\n",
    ": 마지막으로 연산 결과물을 4차원으로 변형하는 것이 합성곱 계층의 구현 흐름 (그림 7-19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.3 합성곱 계층 구현하기\n",
    ": (1단계) 이미지 평탄화<br/>\n",
    ": 전개된 이미지 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "sys.path.append(os.pardir + '/src')\n",
    "\n",
    "from common.util import im2col\n",
    "\n",
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "col1 = im2col(x1, 5, 5, stride = 1, pad = 0)\n",
    "print(col1.shape)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride = 1, pad = 0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": (2단계) 합성곱 계층 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    \n",
    "    def __init__(self, W, b, stride = 1, pad = 0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "        \n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.4 풀링 계층 구현하기\n",
    "\n",
    ": 채널 쪽이 독립적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    \n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        \n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.5 CNN 구현하기\n",
    ": 앞서 구현한 합성곱 계층과 폴링 계층을 조합하여 CNN 구현\n",
    "## SimpleConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size) \n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    \n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "    \n",
    "    \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "            \n",
    "            \n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 데이터 학습\n",
    "## trainConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2991365062851963\n",
      "=== epoch:1, train acc:0.099, test acc:0.088 ===\n",
      "train loss:2.2971986639091067\n",
      "train loss:2.2937726349205185\n",
      "train loss:2.2885589882183663\n",
      "train loss:2.277781255232904\n",
      "train loss:2.267406802992254\n",
      "train loss:2.2551011858700565\n",
      "train loss:2.2324261648794974\n",
      "train loss:2.2129387284028166\n",
      "train loss:2.1768263534530052\n",
      "train loss:2.160874908139011\n",
      "train loss:2.0804182110687273\n",
      "train loss:2.0801311827407085\n",
      "train loss:2.0300631475366306\n",
      "train loss:1.9706791028321264\n",
      "train loss:1.952866399501128\n",
      "train loss:1.8370283819651823\n",
      "train loss:1.7720456767226478\n",
      "train loss:1.6791756984661392\n",
      "train loss:1.6317359447003446\n",
      "train loss:1.54323798164839\n",
      "train loss:1.5148972843949182\n",
      "train loss:1.3673405540204455\n",
      "train loss:1.364037386061838\n",
      "train loss:1.334053929307156\n",
      "train loss:1.1824289648589106\n",
      "train loss:1.1575528972240077\n",
      "train loss:0.9947272884113108\n",
      "train loss:1.0029321569629237\n",
      "train loss:0.9171103381934627\n",
      "train loss:0.822857748043372\n",
      "train loss:0.7967263552780282\n",
      "train loss:0.7916177096263772\n",
      "train loss:0.7774453394964037\n",
      "train loss:0.8370216159382234\n",
      "train loss:0.6659462805481796\n",
      "train loss:0.7671851551586166\n",
      "train loss:0.6666966037420512\n",
      "train loss:0.7038238812438865\n",
      "train loss:0.7432219367473116\n",
      "train loss:0.6105180701994715\n",
      "train loss:0.7169252907593173\n",
      "train loss:0.648737562053896\n",
      "train loss:0.85387607942212\n",
      "train loss:0.6738399400400513\n",
      "train loss:0.5999802174964736\n",
      "train loss:0.717301435529674\n",
      "train loss:0.4993155268366214\n",
      "train loss:0.522604529221336\n",
      "train loss:0.6635785103790707\n",
      "train loss:0.6883028021339431\n",
      "=== epoch:2, train acc:0.802, test acc:0.775 ===\n",
      "train loss:0.4547677306640129\n",
      "train loss:0.680229282790051\n",
      "train loss:0.5366878887212418\n",
      "train loss:0.5209461426377889\n",
      "train loss:0.3902151597779436\n",
      "train loss:0.43259342841692766\n",
      "train loss:0.6454795114342273\n",
      "train loss:0.47918949285405277\n",
      "train loss:0.4932685004194129\n",
      "train loss:0.6665777790381524\n",
      "train loss:0.416120569591471\n",
      "train loss:0.40574317848330566\n",
      "train loss:0.424132275681137\n",
      "train loss:0.4542529922456716\n",
      "train loss:0.49540336588686423\n",
      "train loss:0.41621691995402677\n",
      "train loss:0.6116685912564052\n",
      "train loss:0.3121451311539611\n",
      "train loss:0.41929101923372775\n",
      "train loss:0.41195544805063067\n",
      "train loss:0.5374679839305083\n",
      "train loss:0.32311674109546473\n",
      "train loss:0.4312333721072891\n",
      "train loss:0.349200317355314\n",
      "train loss:0.43093917756954253\n",
      "train loss:0.6706904845328414\n",
      "train loss:0.5594748450244573\n",
      "train loss:0.42331714547003374\n",
      "train loss:0.4564847273183925\n",
      "train loss:0.3016304382890377\n",
      "train loss:0.41222301408078765\n",
      "train loss:0.3249861046157923\n",
      "train loss:0.3975370986432744\n",
      "train loss:0.4766310382302008\n",
      "train loss:0.45266689266864957\n",
      "train loss:0.37362936581402934\n",
      "train loss:0.30997857413158114\n",
      "train loss:0.3787849727978342\n",
      "train loss:0.19444184886643956\n",
      "train loss:0.4033885851312112\n",
      "train loss:0.4936273733671795\n",
      "train loss:0.4323462126200844\n",
      "train loss:0.4228213269464772\n",
      "train loss:0.3723802383027335\n",
      "train loss:0.23219125966431875\n",
      "train loss:0.27432377905126343\n",
      "train loss:0.4564314592813168\n",
      "train loss:0.36619805978987197\n",
      "train loss:0.32616697202336004\n",
      "train loss:0.404795336489288\n",
      "=== epoch:3, train acc:0.878, test acc:0.857 ===\n",
      "train loss:0.30246208016546144\n",
      "train loss:0.20856300645387346\n",
      "train loss:0.333387326334492\n",
      "train loss:0.377068673948835\n",
      "train loss:0.42748899475906305\n",
      "train loss:0.4019236481066142\n",
      "train loss:0.3580119673876624\n",
      "train loss:0.3912650019061001\n",
      "train loss:0.23650272476738066\n",
      "train loss:0.3625570655660785\n",
      "train loss:0.35717065974888745\n",
      "train loss:0.32995382371343424\n",
      "train loss:0.34301962462850405\n",
      "train loss:0.23971723408809345\n",
      "train loss:0.2793717463851634\n",
      "train loss:0.43997071463996607\n",
      "train loss:0.5810940354230971\n",
      "train loss:0.3295369331918164\n",
      "train loss:0.4860918148491724\n",
      "train loss:0.26450988685836724\n",
      "train loss:0.5684215397216168\n",
      "train loss:0.4198155733069667\n",
      "train loss:0.4270937529762322\n",
      "train loss:0.31270610817691646\n",
      "train loss:0.4268835485990488\n",
      "train loss:0.37376142868583656\n",
      "train loss:0.34114483034490506\n",
      "train loss:0.3945246331786148\n",
      "train loss:0.26874695034945395\n",
      "train loss:0.426730497860502\n",
      "train loss:0.48822163808049446\n",
      "train loss:0.22713581290949883\n",
      "train loss:0.3944001518480264\n",
      "train loss:0.2883901031523154\n",
      "train loss:0.3020574244943215\n",
      "train loss:0.24683518311535338\n",
      "train loss:0.20891148131174425\n",
      "train loss:0.1717334525767363\n",
      "train loss:0.21649974262650773\n",
      "train loss:0.33638511471665633\n",
      "train loss:0.27216617515627295\n",
      "train loss:0.5529874535826924\n",
      "train loss:0.37507737817070386\n",
      "train loss:0.4509995324401704\n",
      "train loss:0.4846301556947439\n",
      "train loss:0.31061928250628545\n",
      "train loss:0.39473495927394253\n",
      "train loss:0.2795008779419106\n",
      "train loss:0.33154010841171294\n",
      "train loss:0.24089192944668988\n",
      "=== epoch:4, train acc:0.89, test acc:0.872 ===\n",
      "train loss:0.2083176403914771\n",
      "train loss:0.3857929280669925\n",
      "train loss:0.32142367824875734\n",
      "train loss:0.31941953990727145\n",
      "train loss:0.2719760032489788\n",
      "train loss:0.46372600277589954\n",
      "train loss:0.24786563025837738\n",
      "train loss:0.2989037596744689\n",
      "train loss:0.29063312703161137\n",
      "train loss:0.23567097094237918\n",
      "train loss:0.2259050438410777\n",
      "train loss:0.32496270615604855\n",
      "train loss:0.23652179481335395\n",
      "train loss:0.23846067634073667\n",
      "train loss:0.19768803990652525\n",
      "train loss:0.17783601281703892\n",
      "train loss:0.3593350534821584\n",
      "train loss:0.35946669831858324\n",
      "train loss:0.2983403263515826\n",
      "train loss:0.21425763996545938\n",
      "train loss:0.1834486268108256\n",
      "train loss:0.252834305257103\n",
      "train loss:0.3196343198390194\n",
      "train loss:0.18921024575421838\n",
      "train loss:0.278910462272479\n",
      "train loss:0.24909201541553116\n",
      "train loss:0.22841794951789685\n",
      "train loss:0.31635990773239325\n",
      "train loss:0.29566691491389935\n",
      "train loss:0.3421015684029483\n",
      "train loss:0.22977024474599322\n",
      "train loss:0.38506479657690357\n",
      "train loss:0.21326437474049084\n",
      "train loss:0.1763198600762381\n",
      "train loss:0.270510156119959\n",
      "train loss:0.26017862767442385\n",
      "train loss:0.3164820887828422\n",
      "train loss:0.09368416328706149\n",
      "train loss:0.16846547949623758\n",
      "train loss:0.1836764322847855\n",
      "train loss:0.2895595989850691\n",
      "train loss:0.2035887497954752\n",
      "train loss:0.35123889660117086\n",
      "train loss:0.1670066787766721\n",
      "train loss:0.20732321616038094\n",
      "train loss:0.41563375356491944\n",
      "train loss:0.26059321503095423\n",
      "train loss:0.14627818124599073\n",
      "train loss:0.2903278765620651\n",
      "train loss:0.3114531005528389\n",
      "=== epoch:5, train acc:0.888, test acc:0.896 ===\n",
      "train loss:0.15523632485231667\n",
      "train loss:0.22533429348468836\n",
      "train loss:0.23749298287104773\n",
      "train loss:0.24109776452071838\n",
      "train loss:0.1539793780128169\n",
      "train loss:0.33097177918932713\n",
      "train loss:0.31173246506387814\n",
      "train loss:0.24100446968576303\n",
      "train loss:0.22630890675782647\n",
      "train loss:0.2917338940465918\n",
      "train loss:0.38411007355181775\n",
      "train loss:0.28940195593569246\n",
      "train loss:0.20986436816664422\n",
      "train loss:0.23961833466249297\n",
      "train loss:0.26441986020212027\n",
      "train loss:0.4512712148688412\n",
      "train loss:0.25983493495398774\n",
      "train loss:0.22957694683301427\n",
      "train loss:0.23561055806319547\n",
      "train loss:0.1876927479512983\n",
      "train loss:0.1490788804289526\n",
      "train loss:0.2936392403955441\n",
      "train loss:0.2828218359458799\n",
      "train loss:0.15126547355375747\n",
      "train loss:0.16593183115617754\n",
      "train loss:0.18055222973921226\n",
      "train loss:0.1482627354431139\n",
      "train loss:0.1799508365405117\n",
      "train loss:0.18851793840374875\n",
      "train loss:0.27205134532634384\n",
      "train loss:0.18233623969953597\n",
      "train loss:0.18489774488517013\n",
      "train loss:0.3059509917255202\n",
      "train loss:0.21402324735526845\n",
      "train loss:0.19512339528142808\n",
      "train loss:0.1682722462102868\n",
      "train loss:0.12603189084051056\n",
      "train loss:0.17573500388518196\n",
      "train loss:0.2522065455055707\n",
      "train loss:0.2589779833618926\n",
      "train loss:0.24133434641936152\n",
      "train loss:0.1983677440640242\n",
      "train loss:0.287764480527594\n",
      "train loss:0.27696407009874335\n",
      "train loss:0.1824701434073911\n",
      "train loss:0.2749552766997916\n",
      "train loss:0.2946679992296451\n",
      "train loss:0.3447952626605719\n",
      "train loss:0.27161890293575164\n",
      "train loss:0.3218553012284198\n",
      "=== epoch:6, train acc:0.918, test acc:0.904 ===\n",
      "train loss:0.3001036667490356\n",
      "train loss:0.26842709743153026\n",
      "train loss:0.3190241425435176\n",
      "train loss:0.26052399166741735\n",
      "train loss:0.17956303999341552\n",
      "train loss:0.2467440469879579\n",
      "train loss:0.23022382041669723\n",
      "train loss:0.31897513938575045\n",
      "train loss:0.18941015076109763\n",
      "train loss:0.2567109936964855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.39631335877316426\n",
      "train loss:0.2878443437704456\n",
      "train loss:0.18512288707946453\n",
      "train loss:0.2694793259880066\n",
      "train loss:0.1912845319217999\n",
      "train loss:0.21153196500544783\n",
      "train loss:0.37611662187201367\n",
      "train loss:0.21028603803757118\n",
      "train loss:0.29365224850187804\n",
      "train loss:0.15957184738789154\n",
      "train loss:0.25078913079009596\n",
      "train loss:0.12542317253419263\n",
      "train loss:0.18881963464609872\n",
      "train loss:0.15725230829344208\n",
      "train loss:0.2925955473573014\n",
      "train loss:0.17038091330612812\n",
      "train loss:0.2694107824937441\n",
      "train loss:0.21502740496257317\n",
      "train loss:0.15874455755954042\n",
      "train loss:0.2283162924806647\n",
      "train loss:0.19210053181233885\n",
      "train loss:0.17063072658929473\n",
      "train loss:0.20511858955615242\n",
      "train loss:0.20635064606824907\n",
      "train loss:0.17075906475323432\n",
      "train loss:0.22533500598203002\n",
      "train loss:0.1759179563890327\n",
      "train loss:0.1699947956989163\n",
      "train loss:0.26812511579349424\n",
      "train loss:0.10508929330941194\n",
      "train loss:0.10379132008353548\n",
      "train loss:0.23779641114372208\n",
      "train loss:0.08553352291620721\n",
      "train loss:0.15656619098001515\n",
      "train loss:0.16189631617157157\n",
      "train loss:0.12554954939062335\n",
      "train loss:0.2155175678159381\n",
      "train loss:0.21591482988094066\n",
      "train loss:0.16810875026232458\n",
      "train loss:0.20515465730647947\n",
      "=== epoch:7, train acc:0.929, test acc:0.909 ===\n",
      "train loss:0.23789976471543575\n",
      "train loss:0.09317249598793477\n",
      "train loss:0.2354955740702891\n",
      "train loss:0.2615944814282376\n",
      "train loss:0.17041711397852122\n",
      "train loss:0.1313960079674465\n",
      "train loss:0.23357868757590466\n",
      "train loss:0.15243743918375918\n",
      "train loss:0.053116748437183864\n",
      "train loss:0.14272680168783503\n",
      "train loss:0.1301378613866372\n",
      "train loss:0.15194027801277749\n",
      "train loss:0.11852265094047557\n",
      "train loss:0.18334511715357124\n",
      "train loss:0.07447855016739385\n",
      "train loss:0.1387245905677866\n",
      "train loss:0.18432953985328054\n",
      "train loss:0.2438518438710386\n",
      "train loss:0.0984044165290621\n",
      "train loss:0.08656594941111084\n",
      "train loss:0.1584738786805316\n",
      "train loss:0.12832164670433183\n",
      "train loss:0.1365542521841925\n",
      "train loss:0.09949450515086382\n",
      "train loss:0.2796588392962818\n",
      "train loss:0.14168653125020506\n",
      "train loss:0.20590982649568607\n",
      "train loss:0.10827660716558128\n",
      "train loss:0.1843862763903743\n",
      "train loss:0.14629282461779783\n",
      "train loss:0.12134559521193879\n",
      "train loss:0.16850624216097213\n",
      "train loss:0.1763513924698868\n",
      "train loss:0.1776001443678915\n",
      "train loss:0.17180673653028866\n",
      "train loss:0.16451739116115394\n",
      "train loss:0.18050764230346272\n",
      "train loss:0.152901883458863\n",
      "train loss:0.13786631015880432\n",
      "train loss:0.20510823030826025\n",
      "train loss:0.2140098144200021\n",
      "train loss:0.2095050191856218\n",
      "train loss:0.2549714566026681\n",
      "train loss:0.17890470182913515\n",
      "train loss:0.28031817124312985\n",
      "train loss:0.16497387899301225\n",
      "train loss:0.1020383250026208\n",
      "train loss:0.12869652038150955\n",
      "train loss:0.19876053015875278\n",
      "train loss:0.15776052903688023\n",
      "=== epoch:8, train acc:0.941, test acc:0.918 ===\n",
      "train loss:0.21939243376234482\n",
      "train loss:0.1845491742287809\n",
      "train loss:0.22039102238798203\n",
      "train loss:0.1390567458347635\n",
      "train loss:0.18468693811551756\n",
      "train loss:0.19945083491191415\n",
      "train loss:0.07804552299312068\n",
      "train loss:0.08271945971166254\n",
      "train loss:0.1117430393867459\n",
      "train loss:0.09075122515532548\n",
      "train loss:0.10667700384045925\n",
      "train loss:0.1295293750122315\n",
      "train loss:0.18853133036292127\n",
      "train loss:0.150190828882186\n",
      "train loss:0.1272157570019444\n",
      "train loss:0.11724428937752984\n",
      "train loss:0.18991283541268733\n",
      "train loss:0.11414287232836116\n",
      "train loss:0.11061113325096844\n",
      "train loss:0.17491407293135153\n",
      "train loss:0.25364572996708\n",
      "train loss:0.15784741652780812\n",
      "train loss:0.1568661013250187\n",
      "train loss:0.2227713734443867\n",
      "train loss:0.10773657532820323\n",
      "train loss:0.2097488604253294\n",
      "train loss:0.13574516662816596\n",
      "train loss:0.14427067457922835\n",
      "train loss:0.0924430007617784\n",
      "train loss:0.23614007969784434\n",
      "train loss:0.09307322861779829\n",
      "train loss:0.11673370851739667\n",
      "train loss:0.06458335759362108\n",
      "train loss:0.05106214888295121\n",
      "train loss:0.2078229291656355\n",
      "train loss:0.18321798441822834\n",
      "train loss:0.09908731860680085\n",
      "train loss:0.1592762054135428\n",
      "train loss:0.17608068411950037\n",
      "train loss:0.1311316496972063\n",
      "train loss:0.24381119906976767\n",
      "train loss:0.06847811273733717\n",
      "train loss:0.09688630062362519\n",
      "train loss:0.19626495657267307\n",
      "train loss:0.14260761222750856\n",
      "train loss:0.19817338084777444\n",
      "train loss:0.12830839403641792\n",
      "train loss:0.0894673949283727\n",
      "train loss:0.07601539640792482\n",
      "train loss:0.19373545050638386\n",
      "=== epoch:9, train acc:0.955, test acc:0.93 ===\n",
      "train loss:0.10121626787118995\n",
      "train loss:0.17991410200703772\n",
      "train loss:0.25216116538003536\n",
      "train loss:0.07055036485619903\n",
      "train loss:0.21037680724272387\n",
      "train loss:0.11684469505476264\n",
      "train loss:0.2523853325324815\n",
      "train loss:0.21292674487911953\n",
      "train loss:0.15804574605913543\n",
      "train loss:0.09161140698155176\n",
      "train loss:0.21823358825121741\n",
      "train loss:0.10056784481175281\n",
      "train loss:0.13311641416985598\n",
      "train loss:0.1362118776716479\n",
      "train loss:0.15222587398148055\n",
      "train loss:0.10722252854251219\n",
      "train loss:0.22461262580733365\n",
      "train loss:0.14720171971804605\n",
      "train loss:0.11246894315464603\n",
      "train loss:0.1013970354675417\n",
      "train loss:0.09548625548582347\n",
      "train loss:0.12180891678896036\n",
      "train loss:0.10093037160576196\n",
      "train loss:0.1638444983575553\n",
      "train loss:0.11306796885311515\n",
      "train loss:0.10640361971838957\n",
      "train loss:0.14227511736430892\n",
      "train loss:0.182633847638721\n",
      "train loss:0.05889119741040649\n",
      "train loss:0.06616115853512668\n",
      "train loss:0.19355551285228134\n",
      "train loss:0.16074661372298762\n",
      "train loss:0.057391003867354844\n",
      "train loss:0.10548589281356648\n",
      "train loss:0.08862761263786335\n",
      "train loss:0.2032142932803629\n",
      "train loss:0.13989491764196538\n",
      "train loss:0.09394936452714153\n",
      "train loss:0.08472240530537413\n",
      "train loss:0.07831741059129804\n",
      "train loss:0.11641355251978303\n",
      "train loss:0.1067364955769982\n",
      "train loss:0.05789480667180849\n",
      "train loss:0.09347240320196525\n",
      "train loss:0.222408651193286\n",
      "train loss:0.16854816506830925\n",
      "train loss:0.09640800045240551\n",
      "train loss:0.1302623200287361\n",
      "train loss:0.1505674590825753\n",
      "train loss:0.13011432229756262\n",
      "=== epoch:10, train acc:0.963, test acc:0.934 ===\n",
      "train loss:0.16376797748995475\n",
      "train loss:0.1294104258286527\n",
      "train loss:0.06704913976432415\n",
      "train loss:0.11701048951462602\n",
      "train loss:0.19157372978680198\n",
      "train loss:0.06168846426534132\n",
      "train loss:0.06238554693538879\n",
      "train loss:0.10868744120123594\n",
      "train loss:0.06587746998846915\n",
      "train loss:0.08997749020204138\n",
      "train loss:0.2051396326828252\n",
      "train loss:0.08676750156600303\n",
      "train loss:0.08446140182482242\n",
      "train loss:0.05673635362636813\n",
      "train loss:0.13735624644231412\n",
      "train loss:0.10860835206588307\n",
      "train loss:0.12981286543485246\n",
      "train loss:0.13429110994760504\n",
      "train loss:0.1733293823550173\n",
      "train loss:0.060001290266807984\n",
      "train loss:0.18423863409525512\n",
      "train loss:0.07637639683189022\n",
      "train loss:0.17150507488612887\n",
      "train loss:0.054763361165254515\n",
      "train loss:0.09434343170457524\n",
      "train loss:0.06905454278372367\n",
      "train loss:0.03874352425415831\n",
      "train loss:0.11175659617464641\n",
      "train loss:0.08170377836582712\n",
      "train loss:0.12602879121297575\n",
      "train loss:0.13765641368200757\n",
      "train loss:0.08077538332692102\n",
      "train loss:0.08946110671736542\n",
      "train loss:0.1750274868948425\n",
      "train loss:0.07355165569750914\n",
      "train loss:0.09849494960915348\n",
      "train loss:0.1734871202795152\n",
      "train loss:0.05393313511819116\n",
      "train loss:0.07789666195683828\n",
      "train loss:0.10393696054462305\n",
      "train loss:0.14689942159444278\n",
      "train loss:0.05044330460120548\n",
      "train loss:0.0746573021266464\n",
      "train loss:0.19745792139450022\n",
      "train loss:0.13171766352442113\n",
      "train loss:0.11740107138999115\n",
      "train loss:0.09607720079614084\n",
      "train loss:0.08367856067396906\n",
      "train loss:0.05598805257587614\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.933\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 10\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.6 CNN 시각화하기\n",
    "\n",
    ": 필터의 모습을 보면 가로 방향의 에지나 세로 방향의 에지에 영향을 받는다는 사실을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "network = SimpleConvNet()\n",
    "# 무작위(랜덤) 초기화 후의 가중치\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 학습된 가중치\n",
    "network.load_params(\"params.pkl\")\n",
    "filter_show(network.params['W1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.7 대표적인 CNN\n",
    "## 7.7.1 LeNet\n",
    ": 손글씨 숫자를 인식하는 네트워크, 1998에 제안<br/>\n",
    ": (그림 7-27)<br/>\n",
    ": 단순한 합성곱 계층과 풀링 계층의 반복, 마지막에 완전연결 계층<br/>\n",
    ": 활성화 함수로 Sigmoid 사용<br/>\n",
    ": Sigmoid를 사용하는 경우, Gradient Vanishing 발생할 수 있어, 요즘은 ReLU 사용<br/>\n",
    ": 32x32x1 이미지\n",
    "## 7.7.2 AlexNet\n",
    ": ImageNet 영상 데이터 베이스를 기반으로 한 화상 인식 대회인 ILSVRC 2012에서 우승한 CNN 구조<br/>\n",
    ": 구조는 LeNet과 비슷<br/>\n",
    ": 활성화 함수 ReLU 사용, LRN이라는 국소적 정규화를 실시하는 계층 이용, 드롭아웃 사용<br/>\n",
    ": 2개의 병렬 GPU를 기반으로한 병렬 구조<br/>\n",
    ": 224x224x3 이미지<br/>\n",
    ": GPU-1은 컬러와 상관없는 정보, GPU-2는 컬러와 상관있는 정보<br/>\n",
    "![이미지](images.jpeg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
