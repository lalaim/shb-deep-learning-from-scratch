{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH05\n",
    "\n",
    "| 파일명 | 파일 용도 | 관련 절 | 페이지 |\n",
    "|:--   |:--      |:--    |:--      |\n",
    "| buy_apple.py | 사과 2개를 구입하는 예제의 순전파와 역전파 구현입니다. | 5.4.1 곱셈 계층 | 162 |\n",
    "| buy_apple_orange.py | 사과와 오랜지를 구입하는 예제의 순전파와 역전파 구현입니다. | 5.4.2 덧셈 계층 | 163 |\n",
    "| gradient_check.py | 수치 미분 방식과 비교하여 오차역전파법으로 구한 기울기를 검증합니다(기울기 확인). | 5.7.3 오차역전파법으로 구한 기울기 검증하기 | 184 |\n",
    "| layer_naive.py | 곱셈 계층과 덧셈 계층의 구현입니다. | 5.4.1 곱셈 계층 / 5.4.2 덧셈 계층 | 161, 163 |\n",
    "| train_neuralnet.py | 4장의 train_neuralnet.py와 같습니다. 단, 수치 미분 대신 오차역전파법으로 기울기를 구합니다. | 5.7.4 오차역전파법을 사용한 학습 구현하기 | 186 |\n",
    "| two_layer_net.py | 오차역전파법을 적용한 2층 신경망 클래스 | 5.7.2 오차역전파법을 적용한 신경망 구현하기 | 181 |\n",
    "\n",
    "## 5장 오차역전파법\n",
    "앞 장에서는 신경망 학습에 대해서 설명했습니다. 그때 신경망의 가중치 매개변수의 기울기(정확히는 가중치 매개변수에 대한 손실 함수의 기울기)는 수치 미분을 사용해 구했습니다. 수치 미분은 단순하고 구현하기도 쉽지만 계산 시간이 오래 걸린다는 게 단점입니다. 이번 장에서는 가중치 매개변수의 기울기를 효율적으로 계산하는 ‘오차역전파법backpropagation’을 배워보겠습니다.\n",
    "오차역전파법을 제대로 이해하는 방법은 두 가지가 있을 것입니다. 하나는 수식을 통한 것이고, 다른 하나는 계산 그래프를 통한 것입니다. 전자 쪽이 일반적인 방법으로, 특히 기계학습을 다루는 책 대부분은 수식을 중심으로 이야기를 전개합니다. 확실히 수식을 사용한 설명은 정확하고 간결하므로 올바른 방법이라 할 수 있겠죠. 하지만 졸업 후 너무 오랜만에 수식을 중심으로 생각하다 보면 본질을 놓치거나, 수많은 수식에 당황하는 일이 벌어지기도 합니다. 그래서 이번 장에서는 계산 그래프를 사용해서 ‘시각적’으로 이해시켜드리겠습니다. 그런 다음 실제로 코드를 작성해보면 ‘과연!’이란 탄성과 함께 더 깊이 이해하게 될 것입니다.\n",
    "\n",
    "*옮긴이_ 오차역전파법을 풀어쓰면 ‘오차를 역(반대 방향)으로 전파하는 방법(backward propagation of errors )’입니다. 너무 길고 쓸데없이 어려운 느낌이라 줄여서 ‘역전파법’ 혹은 그냥 ‘역전파’라고 쓰기도 합니다.*\n",
    "\n",
    "## 목차\n",
    "```\n",
    "5.1 계산 그래프 \n",
    "__5.1.1 계산 그래프로 풀다 \n",
    "__5.1.2 국소적 계산 \n",
    "__5.1.3 왜 계산 그래프로 푸는가? \n",
    "5.2 연쇄법칙 \n",
    "__5.2.1 계산 그래프에서의 역전파 \n",
    "__5.2.2 연쇄법칙이란? \n",
    "__5.2.3 연쇄법칙과 계산 그래프 \n",
    "5.3 역전파 \n",
    "__5.3.1 덧셈 노드의 역전파 \n",
    "__5.3.2 곱셈 노드의 역전파 \n",
    "__5.3.3 사과 쇼핑의 예 \n",
    "5.4 단순한 계층 구현하기 \n",
    "__5.4.1 곱셈 계층 \n",
    "__5.4.2 덧셈 계층 \n",
    "5.5 활성화 함수 계층 구현하기 \n",
    "__5.5.1 ReLU 계층 \n",
    "__5.5.2 Sigmoid 계층 \n",
    "5.6 Affine/Softmax 계층 구현하기 \n",
    "__5.6.1 Affine 계층 \n",
    "__5.6.2 배치용 Affine 계층 \n",
    "__5.6.3 Softmax-with-Loss 계층 \n",
    "5.7 오차역전파법 구현하기 \n",
    "__5.7.1 신경망 학습의 전체 그림 \n",
    "__5.7.2 오차역전파법을 적용한 신경망 구현하기 \n",
    "__5.7.3 오차역전파법으로 구한 기울기 검증하기 \n",
    "__5.7.4 오차역전파법을 사용한 학습 구현하기\n",
    "```\n",
    "\n",
    "## 이번 장에서 배운 내용\n",
    "* 계산 그래프를 이용하면 계산 과정을 시각적으로 파악할 수 있다.\n",
    "* 계산 그래프의 노드는 국소적 계산으로 구성된다. 국소적 계산을 조합해 전체 계산을 구성한다.\n",
    "* 계산 그래프의 순전파는 통상의 계산을 수행한다. 한편, 계산 그래프의 역전파로는 각 노드의 미분을 구할 수 있다.\n",
    "* 신경망의 구성 요소를 계층으로 구현하여 기울기를 효율적으로 계산할 수 있다(오차역전파법).\n",
    "* 수치 미분과 오차역전파법의 결과를 비교하면 오차역전파법의 구현에 잘못이 없는지 확인할 수 있다(기울기 확인).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 서론\n",
    "\n",
    "* 이 책에서는 수식 대신 계산 그래프를 사용하여 '시각적'으로 오차역전파법을 이해\n",
    "* 설명 방식 참고\n",
    " - [Andrey Karpathy의 블로그](http://karpathy.github.io/neuralnets/)\n",
    " - [스탠퍼드 CS231n](https://cs231n.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 계산 그래프 \n",
    "\n",
    "# 5.2 연쇄법칙\n",
    "\n",
    "# 5.3 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5.4 단순한 계층 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4.1 곱셈 계층\n",
    "\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # dL / dx = (dL / d(x*y)) * (d(x*y) / dx) = dout * y\n",
    "        dx = dout * self.y\n",
    "        # dL / dy = (dL / d(x*y)) * (d(x*y) / dy) = dout * x\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy\n",
    "    \n",
    "# 예제 코드는 시간상 생략..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4.2 덧셈 계층\n",
    "\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        # 덧셈 계층에서는 곱셈과 달리 순전파때 따로 저장할 내용이 없어 초기화도 생량\n",
    "        pass # 이 함수에서 아무것도 하지말라는 명령\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # dL / dx = (dL / d(x+y)) * (d(x+y) / dx) = dout * 1\n",
    "        dx = dout * 1\n",
    "        # dL / dy = (dL / d(x+y)) * (d(x+y) / dy) = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy\n",
    "    \n",
    "# 예제 코드는 시간상 생략..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 활성화 함수 계층 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "# 5.5.1 ReLU 계층\n",
    "\n",
    "# ReLU는 전기 회로의 '스위치'에 비유할 수 있음.\n",
    "# 순전파때 전류가 흐르면 스위치 ON, 아니면 OFF\n",
    "# 역전파때 스위치가 ON이면 전류를 그대로 보내고, OFF면 보내지 않음\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0) # 순전파 입력인 x가 0 이하인 인덱스는 True, 그 외는 False로 유지 여부 저장\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0 # mask가 참인 인덱스만 0으로 변환\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0 # 입력 x가 0 이하인 부분의 역전파값은 0\n",
    "        dx = dout # 그 외의 곳은 dout * 1\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "# mask 연습\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(x)\n",
    "mask = (x <= 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5.2 Sigmoid 계층\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # 풀이 과정은 복잡하지만, 역전파값이 출력값(y)의 함수로 간단히 표현된다는게 포인트!\n",
    "        # dL / dx = dL / dy * (y * (1 - y))\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Affine/Softmax 계층 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6.1 Affine 계층 (행렬곱 계산 부분)\n",
    "\n",
    "# 5.6.2 배치용 Affine 계층\n",
    "\n",
    "# 교재에 나온 Affine 구현 \n",
    "#\n",
    "# (예제 소스의 Affine 구현(common/layers.py)은 \n",
    "#  텐서(4차원 데이터) 입력도 고려한 것이라 차이가 있음)\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고: 예제 소스에서 구현한 내용\n",
    "\n",
    "# class Affine:\n",
    "#     def __init__(self, W, b):\n",
    "#         self.W = W\n",
    "#         self.b = b\n",
    "        \n",
    "#         self.x = None\n",
    "#         self.original_x_shape = None\n",
    "#         # 가중치와 편향 매개변수의 미분\n",
    "#         self.dW = None\n",
    "#         self.db = None\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # 텐서 대응\n",
    "#         self.original_x_shape = x.shape\n",
    "#         x = x.reshape(x.shape[0], -1)\n",
    "#         self.x = x\n",
    "\n",
    "#         out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "#         return out\n",
    "\n",
    "#     def backward(self, dout):\n",
    "#         dx = np.dot(dout, self.W.T)\n",
    "#         self.dW = np.dot(self.x.T, dout)\n",
    "#         self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "#         dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "#         return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6.3 Softmax-with-Loss 계층\n",
    "\n",
    "# 다소 복잡하지만,\n",
    "# softmax 계층과 loss(Cross Entropy) 계층으로 나눠 간소화할 수 있음\n",
    "\n",
    "# 3.5.2 장에서 구현했던 내용, 여기서는 예제 소스(common/functions.py)에서 가져옴\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "# 4.2.4 장에서 구현했던 내용. 여기서는 예제 소스(common/functions.py)에서 가져옴\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실\n",
    "        self.y = None # softmax의 출력\n",
    "        self.t = None # 정답 레이블 (원-핫 인코딩)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 오차역전파법 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7.2 오차역전파법을 적용한 신경망 구현하기\n",
    "\n",
    "# 4.5 장의 내용과 공통되는 부분이 많음\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# 4장에서 사용했던 수치 미분으로 기울기 구하는 방식\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # OrderedDict에 추가한 순서대로 순전파 수행\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:4.6074628352795184e-10\n",
      "b1:2.7778595757259503e-09\n",
      "W2:6.06562058894024e-09\n",
      "b2:1.4031239723710787e-07\n"
     ]
    }
   ],
   "source": [
    "# 5.7.3 오차역전파법으로 구한 기울기 검증하기\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir) # 부모 디렉토리의 파일을 가져올 수 있도록 설정\n",
    "from src.dataset.mnist import load_mnist\n",
    "\n",
    "np.random.seed(0) # 재현가능용 seed 고정\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 절대 오차의 평균을 구한다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 수치 미분(numerical_gradient)과 오차역전파법(gradient)로 구한 기울기의 차이가 매우 작음\n",
    "\n",
    "-> 오차역전파법으로 구한 기울기도 실수 없이 구현했다는 믿음이 커짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch#000: train acc, test acc | 0.1090, 0.1136\n",
      "epoch#001: train acc, test acc | 0.9051, 0.9069\n",
      "epoch#002: train acc, test acc | 0.9216, 0.9212\n",
      "epoch#003: train acc, test acc | 0.9334, 0.9336\n",
      "epoch#004: train acc, test acc | 0.9450, 0.9430\n",
      "epoch#005: train acc, test acc | 0.9513, 0.9482\n",
      "epoch#006: train acc, test acc | 0.9566, 0.9532\n",
      "epoch#007: train acc, test acc | 0.9614, 0.9572\n",
      "epoch#008: train acc, test acc | 0.9634, 0.9582\n",
      "epoch#009: train acc, test acc | 0.9661, 0.9613\n",
      "epoch#010: train acc, test acc | 0.9697, 0.9627\n",
      "epoch#011: train acc, test acc | 0.9711, 0.9635\n",
      "epoch#012: train acc, test acc | 0.9733, 0.9669\n",
      "epoch#013: train acc, test acc | 0.9744, 0.9662\n",
      "epoch#014: train acc, test acc | 0.9755, 0.9684\n",
      "epoch#015: train acc, test acc | 0.9775, 0.9689\n",
      "epoch#016: train acc, test acc | 0.9786, 0.9699\n",
      "CPU times: user 1min 3s, sys: 2.43 s, total: 1min 6s\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 5.7.4 오차역전파법을 사용한 학습 구현하기\n",
    "# (4장에서 썼던 코드와 거의 똑같아 4장 예제 소스로 참고)\n",
    "\n",
    "# (4.5.2 미니배치 학습 구현하기 (ch04/train_neuralnet.py 참고))\n",
    "\n",
    "import numpy as np\n",
    "from src.dataset.mnist import load_mnist\n",
    "#from two_layer_net import TwoLayerNet\n",
    "\n",
    "np.random.seed(0) # 재현가능용 seed 고정\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "# x_train = x_train[0:100,:]\n",
    "# t_train = t_train[0:100,:]\n",
    "\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000 # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = [] # 성능 평가용 추가\n",
    "test_acc_list = [] # 성능 평가용 추가\n",
    "\n",
    "# 1에폭당 반복 수 (# 성능 평가용 추가)\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    #if (i + 1) % 100 == 0:\n",
    "    #    print('.', end='')\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    #########\n",
    "    # 오차역전파법으로 기울기 구함\n",
    "    # 기울기 계산 (numerical_gradient로 하면 많이 느릴 예정)\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"epoch#%03d: train acc, test acc | %.4f, %.4f\" % (i // iter_per_epoch, train_acc, test_acc))\n",
    "#print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
