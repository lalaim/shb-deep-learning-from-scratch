{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Chapter4 wrap-up\n",
    "\n",
    "    - 신경망 학습의 지표는 손실함수(Loss Function)\n",
    "    - 손실함수에는 MSE(주로 회귀문제에 사용)와 CEE(주로 분류문제에 사용)를 사용한다\n",
    "    - 손실함수의 값이 작아지는 방향으로 가중치 매개변수를 갱신\n",
    "    - 가중치 매개변수를 갱신할 때는 가중치 매개변수의 기울기를 이용, 기울어진 방향으로 가중치값을 반복해서 갱신\n",
    "    - 수치 미분을 이용해서 가중치 매개변수의 기울기를 구할 수 있다\n",
    "    - 수치 미분을 이용한 기울기 계산법은 단순하고 구현하기도 쉽지만(?!) 시간이 오래 걸린다\n",
    "    - 오차역전파법을 통하면 빠르게 구할 수 있다\n",
    "\n",
    "\n",
    "# 5 오차역전파법\n",
    "\n",
    "    - 수식을 통해 이해하는 방법과 계산 그래프를 통해 이해하는 방법\n",
    "    - 이 책에서는 계산 그래프를 통해 시각적으로 쉽게 설명\n",
    "\n",
    "\n",
    "## 5.1 계산그래프\n",
    "\n",
    "      - 노드와 에지로 표현되는 그래프 자료구조로 표현\n",
    "        \n",
    "### 5.1.1 계산 그래프로 풀다\n",
    "\n",
    "        가. 문제 1 : 1개에 100원인 사과를 2개 샀다. 소비세가 10%일 때 지불금액 구하기\n",
    "          - 계산그래프로 풀기\n",
    "          - 사과의 갯수와 소비세를 변수로 취급해서 표기\n",
    "        \n",
    "        나. 문제 2 : 100원짜리 사과 2개, 150원짜리 귤 3개, 소비세 10%일 때 지불금액 구하기\n",
    "          - 문제 1과 동일한 방식으로 계산그래프로 표기해서 풀기\n",
    "        \n",
    "        다.그래프 상 계산은 왼쪽에서 오른쪽으로 진행 (순전파 : forward propagation)\n",
    "\n",
    "### 5.1.2 국소적 계산\n",
    "        \n",
    "        가. 사과 2개 포함한 여러 식품을 구입하는 case\n",
    "          - 사과 제외한 여러 식품의 총 금액은 4,000원\n",
    "          - 사과와 여러 식품의 값을 더하는 계산은 두 가지 숫자만 계산하면 끝\n",
    "          - 각 노드는 자신과 관련된 계산 이외에는 신경쓰지 않아도 됨\n",
    "          - 단계별로 <국소적 계산>을 수행해서 단순한 방식으로 전체 복잡한 계산 가능\n",
    "          \n",
    "### 5.1.3 왜 계산 그래프로 푸는가?\n",
    "\n",
    "        가. 국소적 계산으로 단순하게 풀기 위해\n",
    "        나. 역전파를 통해 <미분>을 효율적으로 계산할 수 있어서\n",
    "        다. 문제 1 case : 사과가격이 오르면 최종 금액에 어떤 영향을 끼치는가?\n",
    "          - 사과 가격에 대한 지불 금액의 미분값을 구하는 문제 (dL / dx)\n",
    "          - 사과 가격이 1 오르면 최종 금액은 2.2배만큼 오른다\n",
    "\n",
    "\n",
    "## 5.2 연쇄법칙\n",
    "\n",
    "      - 역전파는 국소적인 미분을 연쇄법칙에 의해 오른쪽에서 왼쪽으로 전달\n",
    "\n",
    "### 5.2.1 계산 그래프의 역전파\n",
    "\n",
    "        가. case : y = f(x) 함수의 역전파\n",
    "          - 신호 E(상류에서 전달된 값)에 노드의 국소적 미분 (dy/dx)을 곱해서 다음 노드로 전달\n",
    "\n",
    "### 5.2.2 연쇄법칙이란?\n",
    "\n",
    "        가. 합성함수 : 여러 함수로 구성된 함수\n",
    "          - z = (x + y)**2 식은\n",
    "            z = t**2 와 t = x + y 라는 두 개의 식으로 구성\n",
    "        나. 연쇄법칙 : 합성함수의 미분에 대한 성질\n",
    "          - 합성함수의 미분은 합성함수를 구성하는 각 함수의 미분의 곱\n",
    "          - dz/dx = dz/dt * dt/dx\n",
    "          - dz/dt = 2t\n",
    "            dt/dx = 1\n",
    "            dz/dx = dz/dt * dt/dx = 2t * 1 = 2(x + y)\n",
    "            \n",
    "### 5.2.3 연쇄법칙과 계산 그래프\n",
    "\n",
    "        가. 역전파가 하는 일은 <연쇄법칙>의 원리와 동일\n",
    "        \n",
    "        \n",
    "## 5.3 역전파\n",
    "\n",
    "### 5.3.1 덧셈 노드의 역전파\n",
    "\n",
    "        가. case : z = x + y\n",
    "          - dz/dx = 1, dz/dy = 1\n",
    "        나. 덧셈 노드의 역전파는 1을 곱하기만 할 뿐 상류에서 입력된 값을 그대로 다음 노드로 전달\n",
    "\n",
    "### 5.3.2 곱셈 노드의 역전파\n",
    "\n",
    "        가. case : z = xy\n",
    "          - dz/dx = y, dz/dy = x\n",
    "        나. 곱셈 노드의 역전파는 상류에서 입력된 값에 순전파 때의 입력 신호들을 서로 바꿔서 다음 노드로 전달\n",
    "        다. 덧셈 역전파는 순방향 입력 신호값이 필요없고, 곱셈은 순전파의 입력 신호를 변수에 저장\n",
    "        \n",
    "### 5.3.3 사과 쇼핑의 예\n",
    "\n",
    "        가. 사과 쇼핑 case (문제 1)\n",
    "          - 세 변수가 최종 금액에 어떻게 영향을 주는가 (사과 가격, 사과 갯수, 소비세)\n",
    "        나. 사과 & 귤 쇼핑 case (문제 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 단순한 계층 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4.1 곱셈 계층\n",
    "\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y  # x와 y를 바꾼다\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "# 문제 1 : 사과 case\n",
    "\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(price)  # 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "# 역전파\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)  # 2.2 110 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4.2 덧셈 계층\n",
    "\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "# 문제 2 : 사과 & 오렌지 case\n",
    "\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)  #(1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)  #(2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  #(3)\n",
    "price = mul_tax_layer.forward(all_price, tax)  #(4)\n",
    "\n",
    "# 역전파\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)  #(4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  #(3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  #(2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  #(1)\n",
    "\n",
    "print(price)  # 715\n",
    "print(dapple_num, dapple, dorange, dorange_num, dtax)  # 110 2.2 3.3 165 650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 활성화 함수 계층 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5.1 Relu 계층\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array( [[1.0, -0.5], [-2.0, 3.0]] )\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "mask = (x <= 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5.2 Sigmoid 계층\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Affine/Softmax 계층 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "(3,)\n",
      "[0.42187601 0.74008068 0.93810834]\n"
     ]
    }
   ],
   "source": [
    "# 5.6.1 Affine 계층\n",
    "\n",
    "X = np.random.rand(2)    # 입력\n",
    "W = np.random.rand(2,3)  # 가중치\n",
    "B = np.random.rand(3)    # 편향\n",
    "\n",
    "X.shape  # (2,)\n",
    "W.shape  # (2, 3)\n",
    "B.shape  # (3,)\n",
    "\n",
    "Y = np.dot(X, W) + B\n",
    "\n",
    "print(X.shape)\n",
    "print(W.shape)\n",
    "print(B.shape)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0],\n",
       "       [10, 10, 10]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.6.2 배치용 Affine 계층\n",
    "\n",
    "X_dot_W = np.array([[0,0,0], [10, 10, 10]])\n",
    "B = np.array([1, 2, 3])\n",
    "\n",
    "X_dot_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [11, 12, 13]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dot_W + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "dY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dB = np.sum(dY, axis=0)\n",
    "dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6.3 Softmax-with-Loss 계층\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None  # 손실\n",
    "        self.y = None     # Softmax의 출력\n",
    "        self.t = None     # 정답 레이블(one-hot 벡터)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 오차역전파법 구현하기\n",
    "\n",
    "\n",
    "### 5.7.1 신경망 학습의 전체 그림\n",
    "\n",
    "        가. 전제\n",
    "          - 신경망에는 적응 가능한 가중치와 편향이 있고\n",
    "          - 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 <학습>이라 한다\n",
    "          - 신경망 학습은 다음의 4단계로 수행\n",
    "        \n",
    "        나. 1단계 : 미니배치\n",
    "          - 훈련 데이터 중 일부를 random으로 발췌(미니배치), 미니배치의 손실 함수 값 최소화가 목표\n",
    "        \n",
    "        다. 2단계 : 기울기 산출\n",
    "          - 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수 기울기 산출\n",
    "          - 기울기는 손실 함수 값을 가장 작게 하는 방향을 제시\n",
    "          \n",
    "        라. 3단계 : 매개변수 갱신\n",
    "          - 가중치 매개변수를 기울기 방향으로 아주 조금씩 갱신\n",
    "          \n",
    "        마. 4단계 : 반복\n",
    "          - 1~3단계 반복\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7.2 오차역전파법을 적용한 신경망 구현하기\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "\n",
    "from src.common.layers import *\n",
    "from src.common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "                 weight_init_std=0.01):\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "            return x\n",
    "        \n",
    "    # x : 입력데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = ()\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        \n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:3.675225363121541e-10\n",
      "b1:2.2597084914886823e-09\n",
      "W2:4.82734310998579e-09\n",
      "b2:1.395743221985213e-07\n"
     ]
    }
   ],
   "source": [
    "### 5.7.3 오차역전파법으로 구한 기울기 검증하기\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from src.dataset.mnist import load_mnist\n",
    "from src.ch05.two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 차이의 절댓값을 구한 후, 그 절댓값들의 평균을 낸다\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12438333333333333 0.121\n",
      "0.9014666666666666 0.9043\n",
      "0.9228166666666666 0.924\n",
      "0.9357 0.9359\n",
      "0.943 0.9428\n",
      "0.9518166666666666 0.9499\n",
      "0.9561 0.9536\n",
      "0.9601666666666666 0.9574\n",
      "0.9635833333333333 0.9606\n",
      "0.9671 0.9615\n",
      "0.9689 0.9644\n",
      "0.9708833333333333 0.9648\n",
      "0.9722833333333334 0.9649\n",
      "0.9744833333333334 0.9679\n",
      "0.9758166666666667 0.9674\n",
      "0.9767166666666667 0.969\n",
      "0.9785166666666667 0.9704\n"
     ]
    }
   ],
   "source": [
    "# 5.7.4 오차역전파법을 사용한 학습 구현하기\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from src.dataset.mnist import load_mnist\n",
    "from src.ch05.two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 오차역전파법으로 기울기를 구한다\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
